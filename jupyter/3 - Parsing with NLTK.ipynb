{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "try:\n",
    "    directory = sys.args[1]\n",
    "except:\n",
    "    directory = '../review_json/'\n",
    "\n",
    "def load_reviews(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    reviews = [json.loads(open(directory+fn, 'r').read()) for fn in file_names]\n",
    "    reviews.sort(key=lambda r:len(r['text'])) # sort by length of review\n",
    "    reviews = reviews[5:] # get rid of 5 shortest reviews\n",
    "    for r in reviews:\n",
    "        r.pop('by') # remove 'by' field in each review\n",
    "    return reviews\n",
    "\n",
    "reviews = load_reviews(directory)\n",
    "reviews.sort(key=lambda r:r['release_year']) # sort by release_year\n",
    "titles = {r['title']:r for r in reviews} # dict of reviews by title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "mr = titles['Minority Report (2002)'] # the Minority Report review\n",
    "mr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text = mr['text']\n",
    "\n",
    "# nltk is a language processing toolkit\n",
    "# nltk?\n",
    "# nltk.[TAB]... to explore the package\n",
    "\n",
    "# let's analyze sentences\n",
    "mr_sents = nltk.sent_tokenize(mr_text) # segment into sentences\n",
    "\n",
    "# sort and display sentences by length\n",
    "mr_sents.sort(key=lambda x:len(x)) # shortest to longest\n",
    "mr_sents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents[-5:]\n",
    "# note we can no longer recover the original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.word_tokenize(mr_sents[0]) # ['The', 'year', 'is', '2054', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_tokens = nltk.word_tokenize(mr['text']) # not organized into sentences\n",
    "mr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sent_tokens = []\n",
    "for sent in nltk.sent_tokenize(mr_text):\n",
    "    sent_tokens = nltk.word_tokenize(sent)\n",
    "    mr_sent_tokens.append(sent_tokens)\n",
    "\n",
    "mr_sent_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sent_tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's look at the nltk Text class, which wrappers a set of tokens\n",
    "# nltk.Text?\n",
    "\"\"\"\"\n",
    "A wrapper around a sequence of simple (string) tokens, which is\n",
    "intended to support initial exploration of texts (via the\n",
    "interactive console).  Its methods perform a variety of analyses\n",
    "on the text's contexts (e.g., counting, concordancing, collocation\n",
    "discovery), and display the results.  If you wish to write a\n",
    "program which makes use of these analyses, then you should bypass\n",
    "the ``Text`` class, and use the appropriate analysis function or\n",
    "class directly instead.\n",
    "\"\"\"\n",
    "mr_wrapper = nltk.Text(mr_tokens)\n",
    "mr_wrapper.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.count('action') # counts tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.count('he') # if we don't have tokens, number of 'he' pronouns incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.count('he') # correct 'he' count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.count(' he ') # would miss \"he's\" and '--he' and 'he,' ... so not trivial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_vocab = mr_wrapper.vocab() # unique tokens with their counts\n",
    "mr_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_vocab.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_fdist = nltk.FreqDist(mr_wrapper)\n",
    "mr_fdist.most_common(50) # same as mr_vocab.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_fdist.plot() # power law--some tokens vert common but most appear once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ngrams are patterns of tokens with a window size of \"n\"\n",
    "mr_bigrams = list(nltk.ngrams(mr_tokens, 2))\n",
    "mr_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(mr_bigrams).most_common(30) # most common adjacent token pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(list(nltk.ngrams(mr_tokens, 3))).most_common(20) # trigrams less interesting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(list(nltk.ngrams(mr_tokens, 4))).most_common(20) # nor are quadgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Google Books Ngram Viewer: https://books.google.com/ngrams\n",
    "try: \"true love\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocation\n",
    "A sequence of words or terms which co-occur more often than would be expected by chance.\n",
    "'Crystal clear', 'middle management', 'nuclear family', and 'cosmetic surgery' are examples of collocated pairs of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.collocations(num=20, window_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concordance is a list of all the occurrences of a token with context\n",
    "mr_wrapper.concordance('future')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.concordance('action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.collocations(num=20, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.concordance('term', width=200) # using the term \"pre-crime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_wrapper.concordance('spiders', width=200) # \"spiders that can search\"\n",
    "# note that this is a very small sample of text on which to analyze collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stemming: normalizing text\n",
    "# stemming is good for finding the stem / base / root of a word form\n",
    "# a series of if-then transformation rules are applied, typically\n",
    "# from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('happens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('happened')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('happening')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('happenings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('happen') # can't be simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('runnings') # works on made up words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('lemmings') # 'lem' is an error--stemmer has no word knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('plead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer.stem('pled') # doesn't know this is same as 'plead'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(set(mr_tokens)) # 553 unique tokens like 'year' and 'years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's find the tokens that are altered by stemming:\n",
    "stemmed = set([(tok, stemmer.stem(tok)) for tok in mr_tokens if stemmer.stem(tok) != tok]) # actually around 495 unique word stems\n",
    "stemmed # 'worked', 'working', and 'works' become 'work'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lemmatization with a vocabulary: WordNet\n",
    "# lemmatization use a dictionary to reduce tokens to their base forms\n",
    "lemmer = nltk.WordNetLemmatizer()\n",
    "lemmatized = set([(tok, lemmer.lemmatize(tok)) for tok in mr_tokens if lemmer.lemmatize(tok) != tok])\n",
    "lemmatized # tries -> try requires a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's find the set of unique stems, disregarding punctuation marks\n",
    "mr_vocab = set([stemmer.stem(tok.lower()) for tok in mr_tokens if tok.isalpha()])\n",
    "len(mr_vocab) # 464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dictionaries\n",
    "# get rid of trivial words with nltk.corpus.stopwords.words('english')\n",
    "# get rid of general vocabulary with nltk.corpus.words.words()\n",
    "stopwords = [stemmer.stem(word) for word in nltk.corpus.stopwords.words('english')]\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_content = [tok for tok in mr_vocab if tok not in stopwords]\n",
    "len(mr_content) # 390 or 381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_content # content word stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk_vocab = nltk.corpus.words.words()\n",
    "english_vocab = [stemmer.stem(word.lower()) for word in nltk_vocab]\n",
    "len(english_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "random.sample(english_vocab, 20) # some setms from the nltk words corpus\n",
    "mr_unusual = [word for word in mr_content if word not in english_vocab]\n",
    "mr_unusual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'glitch' in nltk_vocab # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'cartoonish' in nltk_vocab # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'filmmaker' in nltk_vocab # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part-Of-Speech (POS) tagging\n",
    "# also known as grammatical tagging\n",
    "# e.g. noun, verb, adjective, adverb, preposition\n",
    "# once done by hand, as in school, now done algorithmically\n",
    "# done by analyzing token character features and surrounding context:\n",
    "nltk.help.upenn_tagset() # https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag = nltk.pos_tag\n",
    "tag(['they','bug','him']) # 'bug' is VBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['a','bug','crawls']) # 'bug' is NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['bug']) # NN most likely use without context information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['wug']) # NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['wugs']) # NNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['it','wugs','well']) # VBZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['wugged']) # VBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tag(['wugging']) # VBG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_pos = tag(mr_tokens)\n",
    "sorted(set(mr_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's look at our tokens by pos label\n",
    "# from collections import defaultdict\n",
    "tokens_by_pos = defaultdict(set)\n",
    "for token, pos in mr_pos:\n",
    "    tokens_by_pos[pos].add(token)\n",
    "\n",
    "sorted(tokens_by_pos) # the labels that appears in the Minority Report tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(tokens_by_pos.items()) # the sets of tokens for each pos label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# noun chunks\n",
    "# NN\tnoun, singular        'desk'\n",
    "# NNS\tnoun plural           'desks'\n",
    "# NNP\tproper noun, singular 'Harrison'\n",
    "# NNPS\tproper noun, plural   'Americans'\n",
    "noun_chunks = defaultdict(list)\n",
    "chunk = []\n",
    "pos_pattern = []\n",
    "for token, pos in mr_pos:\n",
    "    if pos.startswith('N'):\n",
    "        chunk.append(token)\n",
    "        pos_pattern.append(pos)\n",
    "    elif chunk:\n",
    "        noun_chunks[' '.join(pos_pattern)].append(chunk)\n",
    "        pos_pattern, chunk = [], []\n",
    "\n",
    "sorted(noun_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(noun_chunks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: can skip to chunk_parser\n",
    "# noun phrases--a crude attempt to extract them\n",
    "noun_phrases = defaultdict(list)\n",
    "phrase = []\n",
    "pos_pattern = []\n",
    "flag = False\n",
    "for token, pos in mr_pos:\n",
    "    if pos.startswith('N'):\n",
    "        phrase.append(token)\n",
    "        pos_pattern.append(pos)\n",
    "        flag = True\n",
    "    elif pos.startswith('J') and not flag:\n",
    "        phrase.append(token)\n",
    "        pos_pattern.append(pos)\n",
    "    elif phrase:\n",
    "        if pos_pattern[-1].startswith('N'):\n",
    "            noun_phrases[' '.join(pos_pattern)].append(phrase)\n",
    "        pos_pattern, phrase, flag = [], [], False\n",
    "\n",
    "sorted(noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(noun_phrases.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chunking with Regular Expressions\n",
    "grammar = r\"\"\"\n",
    "NP: {<JJ>*<NNS?>+}   # chunk determiner/possessive, adjectives and noun\n",
    "    {<NNPS?>+}                # chunk sequences of proper nouns\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(grammar)\n",
    "mr_parsed = chunk_parser.parse(mr_pos)\n",
    "print(mr_parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_chunks = [chunk for chunk in mr_parsed if type(chunk)==nltk.tree.Tree]\n",
    "mr_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_chunks.sort(key=len) # to see longest phrases chunked\n",
    "mr_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stanford Parser\n",
    "# http://nlp.stanford.edu:8080/parser/index.jsp\n",
    "# see how it tags and organizes text into trees\n",
    "# use smaller text samples\n",
    "\n",
    "# named entity recognition (NER)\n",
    "mr_ne = nltk.ne_chunk(mr_pos, binary=True)\n",
    "list(mr_ne) # \"Tree\" objects contain named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(mr_ne)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: one last review of key parsing tools\n",
    "sentences = nltk.sent_tokenize(mr_text)\n",
    "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "tagged_sentences = [nltk.pos_tag(sentence) for sentence in tokenized_sentences]\n",
    "chunked_sentences = [nltk.ne_chunk(sentence, binary=True) for sentence in tagged_sentences]\n",
    "entities = [chunk for sentence in chunked_sentences for chunk in sentence if type(chunk)!=tuple]\n",
    "list(entities)\n",
    "sorted(entities)\n",
    "\n",
    "# Stanford Named Entity Tagger\n",
    "# http://nlp.stanford.edu:8080/ner/process\n",
    "# paste text from NYTimes to demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
