{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import re\n",
    "import statistics as stats\n",
    "from collections import Counter\n",
    "\n",
    "directory=\"../review_json/\"\n",
    "\n",
    "def load_reviews(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    reviews = [json.loads(open(directory+fn, 'r').read()) for fn in file_names]\n",
    "    reviews.sort(key=lambda r:len(r['text'])) # sort by length of review\n",
    "    reviews = reviews[5:] # get rid of 5 shortest reviews\n",
    "    for r in reviews:\n",
    "        r.pop('by') # remove 'by' field in each review\n",
    "    return reviews\n",
    "\n",
    "def display(r):\n",
    "    print(\"\"\"\n",
    "    Review: %s\n",
    "    ------------------------------\n",
    "    Title: %s\n",
    "    Release year: %s\n",
    "    Running time: %s\n",
    "    MPAA rating: %s\n",
    "    Genres: %s\n",
    "    Star rating: %s\n",
    "    Date Published: %s\n",
    "    Review URL: %s\n",
    "    \"\"\" % (\n",
    "    r['text'],r['title'],r['release_year'],r['running_time'],r['mpaa_rating'],\n",
    "    ', '.join(r['genres']) if r['genres'] else 'N/A',\n",
    "    r['star_rating'],r['date_published'],r['url']\n",
    "    ))\n",
    "\n",
    "reviews = load_reviews(directory)\n",
    "reviews.sort(key=lambda r:r['release_year']) # sort by release_year\n",
    "titles = {r['title']:r for r in reviews} # dict of reviews by title\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr = titles['Minority Report (2002)'] # the Minority Report review\n",
    "mr\n",
    "display(mr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(mr.keys()) # see review fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(mr.items()) # view key-value pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspecting the text with built-in Python functions\n",
    "mr_text = mr['text']\n",
    "print(mr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mr_text) # character count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.count('Pre-Cogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.count('Anderton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.count('the') # 78 occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mr_text.lower()) # normalize letter casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.lower().count('the') # but 88 occurrences in lowercased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mr_text.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.upper().count('THE') # and 88 occurrences in uppercased text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect paragraphs\n",
    "print(mr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_paras = mr_text.split('\\n\\n')\n",
    "len(mr_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_paras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_paras[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_paras.sort(key=lambda p:len(p)) # sort by character length of paragraph\n",
    "mr_paras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_paras[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mr_paras[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mr_paras[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect sentences\n",
    "mr_sents = mr_text.split('. ')\n",
    "mr_sents[:5] # uh oh--newlines are not properly split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents = mr_text.split('.')\n",
    "mr_sents[:5] # looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents[-5:] # looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents[18:22] # ellipsis results in error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text[3000:3200] # the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "mr_sents = re.split('[\\.\\!\\?]\\s', mr_text) # but we lose that punctuation information\n",
    "mr_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents = re.split('([^\\.\\!\\?]+[\\.\\!\\?]\\s+)', mr_text) # group sentences\n",
    "mr_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents[44:46] # but what about quotation marks and parentheticals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents = [sent for sent in mr_sents if sent] # clear empty strings\n",
    "len(mr_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents.sort(key=lambda s:len(s)) # sort by sentence character length\n",
    "mr_sents[:10] # shortest sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_sents[-3:] # longest sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mr_sents[0]) # character length of shortest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(mr_sents[-1]) # character length of longest sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_lengths = [len(sent) for sent in mr_sents]\n",
    "# import statistics as stats\n",
    "stats.mean(sent_lengths) # 132 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.median(sent_lengths) # 131 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.stdev(sent_lengths) # 68 characters\n",
    "# this is setup for computing reading level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inspect tokens (words and punctuation)\n",
    "mr_text.split() # simplest possible approach to English tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mr_text.split(' ') # error is that punctuation and whitespaces glom to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting with regular expressions\n",
    "# import re\n",
    "re.split(' ', mr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re.split('\\s', mr_text) # splits on whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = re.split('(\\W)', mr_text) # groups on non-word characters\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [tok for tok in tokens if tok.strip()] # filters spaces\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokens) # 1391"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens.sort(key=lambda tok:len(tok)) # sort by token length\n",
    "tokens[:10] # shortest tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens[-10:] # longest tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "token_counts = Counter(tokens) # get counts of each token\n",
    "token_counts.most_common(50) # see most common tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts['film']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts['movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts['scene']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counts['character']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(tokens) # alphabetical list of all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_set = set(tokens) # unique tokens\n",
    "sorted(token_set) # alphabetical list of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokens) # count of all tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(token_set) # count of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(token_set) / len(tokens) # lexical diversity 0.3989935298346513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tokens) / len(token_set) # average token occurrences (2.5) misleading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import statistics as stats\n",
    "stats.median(token_counts.values()) # 1 (most tokens appear once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(token_counts.values()) # 69 (but some tokens occur very frequently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_lengths = [len(token) for token in tokens] # token character lengths\n",
    "min(token_lengths) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(token_lengths) # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.mean(token_lengths) # 4.020848310567937"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.median(token_lengths) # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.mode(token_lengths) # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats.stdev(token_lengths) # 2.6701649504450606"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(token_lengths)[-20:] # character lengths of 20 longest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finale: compute reading level / text complexity\n",
    "# https://en.wikipedia.org/wiki/Readability\n",
    "# Most algorithms use word lists and syllable counts\n",
    "# Ours: \"The Code4Lib Readability Index\" uses no extra data\n",
    "words = [tok for tok in re.split('(\\W)', mr_text) if tok.isalpha()]\n",
    "sents = re.split('[\\.\\!\\?]\\s', mr_text)\n",
    "tokenized_sents = []\n",
    "# let's created a list of tokenized sentences\n",
    "for sent in sents:\n",
    "    sent_tokens = [tok for tok in re.split('(\\W)', sent) if tok.isalpha()]\n",
    "    tokenized_sents.append(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_sent_length = stats.mean([len(sent) for sent in tokenized_sents]) # 24 words\n",
    "avg_sent_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_word_length = stats.mean([len(word) for word in words]) # 4.6 characters\n",
    "avg_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leveler(avg_word_length, avg_sent_length):\n",
    "    reading_level = -5 + 2*avg_word_length + 0.25*avg_sent_length\n",
    "    return reading_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's test our leveler before applying it to our Minority Report text\n",
    "# big words, long sentences\n",
    "leveler(5, 30) # 12.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# smaller words, long sentences\n",
    "leveler(4, 30) # 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# medium words, medium sentences\n",
    "leveler(4,20) # 8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# short words, medium sentences\n",
    "leveler(3,20) # 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# short words, short sentences\n",
    "leveler(3,10) # 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minority Report: (4.6, 24)\n",
    "leveler(avg_word_length, avg_sent_length) # 10.147311027415258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's turn this into a function that accepts a text input\n",
    "def leveler(text):\n",
    "    words = [tok for tok in re.split('(\\W)', text) if tok.isalpha()]\n",
    "    sents = re.split('[\\.\\!\\?]\\s', text)\n",
    "    tokenized_sents = []\n",
    "    for sent in sents:\n",
    "        sent_tokens = [tok for tok in re.split('(\\W)', sent) if tok.isalpha()]\n",
    "        tokenized_sents.append(sent_tokens)\n",
    "    avg_sent_length = stats.mean([len(sent) for sent in tokenized_sents]) # 23 words\n",
    "    avg_word_length = stats.mean([len(word) for word in words]) # 4.6 characters\n",
    "    reading_level = -5 + 2*avg_word_length + 0.25*avg_sent_length\n",
    "    return reading_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leveler(mr_text) # yes, 10.147311027415258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's sort our reviews by reading level\n",
    "reviews.sort(key=lambda r:leveler(r['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leveler(reviews[0]['text']) # 4.966795091324202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(reviews[0]) # short sentences with simple language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(reviews[1]) # repartee dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leveler(reviews[-1]['text']) # 28.858155204460967"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(reviews[-1]) # erroneous commas at the end of each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews[-1]['url'] # visit http://www.rogerebert.com/reviews/switch-1991"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "leveler(reviews[-2]['text']) # 18.722480395004354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(reviews[-2]) # long, complex sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
